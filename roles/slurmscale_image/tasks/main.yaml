---

- name: Disable SELinux
  ansible.posix.selinux:
    state: disabled

- name: Ensure firewalld is not installed
  ansible.builtin.dnf:
    name: firewalld
    state: absent

# this should've already been done in the spawn play
- name: Remove defualt modules script
  ansible.builtin.file:
    path: /etc/profile.d/z99-lmod-default-modules.sh
    state: absent

- name: Ensure Lmod is not installed
  ansible.builtin.dnf:
    name: Lmod
    state: absent

# /software is mounted as a systemd automount configured via /etc/fstab. The Ansible mount module (at least in
# ansible.posix 1.5.4) appears to not understand this, and regular `umount` has to be run twice (and can still sometimes
# fail to fully unmount), thus we systemd-umount first. This does not fail if run repeatedly.
- name: systemd-umount /software
  ansible.builtin.command: systemd-umount /software

- name: Remove /software mount
  ansible.posix.mount:
    path: /software
    state: absent

- name: Configure Ceph
  ansible.builtin.import_role:
    name: galaxyproject.general.ceph_mount

- name: Import CVMFS role
  ansible.builtin.import_role:
    name: galaxyproject.cvmfs

- name: Import Slurm role
  ansible.builtin.import_role:
    name: galaxyproject.slurm

- name: Fix /var/spool/slurm perms
  ansible.builtin.file:
    path: /var/spool/slurm
    owner: slurm
    group: slurm
    mode: "0755"

# on restarts we want to wait until the playbook copies slurm.conf and starts slurmd itself
- name: Disable slurmd autostart
  ansible.builtin.service:
    name: slurmd
    enabled: false

# docker is already installed in the feature image but if it wasn't you'd need to do it here
#- name: Install Docker
#  import_role:
#    name: docker

- name: Prune docker
  community.docker.docker_prune:
    containers: true
    images: true
    images_filters:
      dangling: false
    networks: true
    volumes: true
    builder_cache: true

- name: Prefetch docker images
  community.docker.docker_image:
    name: "{{ item }}"
    source: pull
  loop: "{{ slurmscale_docker_images | default([]) }}"
  when: enable_docker

- name: Disable docker service and socket
  ansible.builtin.systemd_service:
    name: "{{ item }}"
    masked: true
  loop:
    - docker.socket
    - docker.service
  when: not enable_docker

# groups are passed from the controller, so the docker gid must match the controller, but precreated groups in the
# feature image make this tricky
- name: Set docker gid
  ansible.builtin.group:
    name: docker
    gid: "{{ docker_gid }}"

- name: Tailscale block
  when: install_tailscale
  block:

    - name: Install Tailscale
      include_role:
        name: artis3n.tailscale.machine

    # This causes a tailscale logout on shutdown so we don't have to do it by hand
    - name: Configure tailscale state=mem
      ansible.builtin.lineinfile:
        path: /etc/default/tailscaled
        regexp: '^FLAGS='
        line: 'FLAGS="--state=mem:"'

    - name: Install galaxy-job-execution
      pip:
        name: "{{ galaxy_job_execution_pip_name | default('galaxy-job-execution') }}"
        extra_args: --extra-index-url=https://wheels.galaxyproject.org/
        virtualenv: /opt/galaxy-job-execution
        virtualenv_command: /usr/bin/python3.9 -m venv

- name: Nvidia driver block
  when: install_nvidia_non_grid_driver
  block:

    - name: Remove GRID driver
      ansible.builtin.dnf:
        name: nvidia-linux-grid-535
        state: absent

    - name: Install Nvidia repo
      ansible.builtin.command: dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo

    - name: Install Nvidia driver
      ansible.builtin.command: dnf module install -y -q nvidia-driver:565-open

    - name: Enable nvidia-persistenced
      ansible.builtin.systemd:
        name: nvidia-persistenced.service
        enabled: true
